{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3QpOqwplMkH2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Wine dataset, available from the UCI Machine Learning Repository, is a classic dataset aimed at classifying wine samples based on their chemical properties. This dataset contains three different classes, with each sample described by 13 different chemical features. The reason for choosing the Wine dataset is its diverse set of 178 wine samples with various chemical properties, providing sufficient data for the model to learn relationships between different features. The inclusion of three different classes makes it an appropriate example for understanding and applying multi-class classification problems. Additionally, it is directly related to real-world applications such as classifying wine quality and types.\n",
        "\n",
        "The Wine dataset typically does not contain missing values and is well-documented, minimizing preprocessing steps and allowing focus on model training. This dataset can be easily loaded and used with the load_wine() function from the sklearn library. It includes 13 chemical features (e.g., alcohol, flavonoids, proline content) and three different wine classes (Class_0, Class_1, Class_2). In summary, the Wine dataset is an ideal choice for evaluating the performance of our models and obtaining applicable results for real-world problems."
      ],
      "metadata": {
        "id": "SD8vHUsSM1kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define multiscope scaler\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42)\n",
        "\n",
        "\n",
        "# Create the AdaBoost mode\n",
        "class AdaBoostMLP(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, base_estimator=None, n_estimators=50, random_state=None):\n",
        "        self.base_estimator = base_estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.random_state = random_state\n",
        "        self.models = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        np.random.seed(self.random_state)\n",
        "        self.models = []\n",
        "        weights = np.ones(X.shape[0]) / X.shape[0]\n",
        "        for _ in range(self.n_estimators):\n",
        "            model = self.base_estimator\n",
        "            indices = np.random.choice(np.arange(X.shape[0]), size=X.shape[0], p=weights)\n",
        "            model.fit(X[indices], y[indices])\n",
        "            predictions = model.predict(X)\n",
        "            incorrect = (predictions != y)\n",
        "            error = np.dot(weights, incorrect) / np.sum(weights)\n",
        "            alpha = 0.5 * np.log((1 - error) / (error + 1e-10))\n",
        "            weights = weights * np.exp(-alpha * y * predictions)\n",
        "            weights /= np.sum(weights)\n",
        "            self.models.append((model, alpha))\n",
        "\n",
        "    def predict(self, X):\n",
        "        model_predictions = np.array([model.predict(X) for model, alpha in self.models])\n",
        "        weighted_predictions = np.zeros(model_predictions.shape[1])\n",
        "        for model, alpha in self.models:\n",
        "            weighted_predictions += alpha * model.predict(X)\n",
        "        return np.sign(weighted_predictions)\n",
        "\n",
        "# Create and train AdaBoost model\n",
        "ada_boost_mlp = AdaBoostMLP(base_estimator=MLPClassifier(hidden_layer_sizes=(1,), max_iter=1000, random_state=42), n_estimators=50, random_state=42)\n",
        "ada_boost_mlp.fit(X_train, y_train)\n",
        "\n",
        "# Make a guess\n",
        "y_pred_ada_mlp = ada_boost_mlp.predict(X_test)\n",
        "\n",
        "# Report results\n",
        "print(\"AdaBoost with MLP Classifier\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_ada_mlp))\n",
        "print(classification_report(y_test, y_pred_ada_mlp))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbmK-0WJNs52",
        "outputId": "85434d7e-5475-4726-c71a-0a96b6fc8a1e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost with MLP Classifier\n",
            "Accuracy: 0.35185185185185186\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      1.00      0.52        19\n",
            "           1       0.00      0.00      0.00        21\n",
            "           2       0.00      0.00      0.00        14\n",
            "\n",
            "    accuracy                           0.35        54\n",
            "   macro avg       0.12      0.33      0.17        54\n",
            "weighted avg       0.12      0.35      0.18        54\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation and Advantages of the Model**\n",
        "\n",
        "AdaBoost is an ensemble method aimed at combining weak learners to create a strong classifier. In this section, we applied the AdaBoost algorithm using a multi-layer perceptron (MLP) as the base classifier. An MLP is an artificial neural network model consisting of neurons, where each neuron computes an output by applying certain weights to the input values. AdaBoost, on the other hand, increases the weights of misclassified samples in each iteration, aiming to improve the model's accuracy.\n",
        "\n",
        "**Advantages**\n",
        "\n",
        "Flexibility and Power: MLP can learn complex, non-linear relationships. When combined with AdaBoost, the model can capture various patterns more effectively.\n",
        "Adaptive Learning: AdaBoost adjusts the weights of misclassified samples in each iteration, improving the model's performance on these samples.\n",
        "Ensemble Approach: AdaBoost increases overall performance by combining many weak learners.\n",
        "Results and Evaluation:\n",
        "\n",
        "The model was trained and tested on the Wine dataset. However, the results showed lower-than-expected accuracy and classification performance:\n",
        "\n",
        "\n",
        "**AdaBoost with MLP Classifier**\n",
        "\n",
        "Accuracy: 0.35185185185185186\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.35      1.00      0.52        19\n",
        "           1       0.00      0.00      0.00        21\n",
        "           2       0.00      0.00      0.00        14\n",
        "\n",
        "    accuracy                           0.35        54\n",
        "    macro avg       0.12      0.33      0.17        54\n",
        "    weighted avg       0.12      0.35      0.18        54\n",
        "**Evaluation of Results**\n",
        "\n",
        "**Low Performance:** The model's accuracy was calculated to be 35%, indicating that the model's predictions were largely incorrect.\n",
        "\n",
        "**Imbalanced Classification:** While the recall for class 0 was high, the recall values for class 1 and class 2 were 0.00. This indicates that the model failed to classify these classes correctly.\n",
        "\n",
        "**F1-Score:** The low F1-scores indicate that the model's overall performance was poor and unsuccessful in the classification task.\n",
        "\n",
        "**Recommendations**\n",
        "\n",
        "**Data Imbalance**: If there is an imbalance among the classes in the dataset, techniques such as data sampling or class weighting can be used.\n",
        "\n",
        "**Model Parameters:** Optimizing the parameters of the MLP and AdaBoost can improve performance.\n",
        "\n",
        "**Alternative Methods:** Other ensemble methods or directly using MLP may be considered instead of AdaBoost.\n",
        "\n",
        "Using AdaBoost with MLP to create a model is theoretically a powerful method, but in this application, it did not perform as expected. To achieve better results, improvements in the model and data processing steps are necessary."
      ],
      "metadata": {
        "id": "t7nq63pbZGcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ql66Z1lLQhE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "class PerceptronNode(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self):\n",
        "        self.model = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.model.predict_proba(X)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "\n",
        "# Create Random Decision Forest and use Perceptron on each node\n",
        "\n",
        "class RandomForestWithPerceptron(RandomForestClassifier):\n",
        "    def _build_tree(self, *args, **kwargs):\n",
        "        tree = super()._build_tree(*args, **kwargs)\n",
        "        tree.estimator = PerceptronNode()\n",
        "        return tree\n",
        "\n",
        "# Create the model\n",
        "\n",
        "rf_with_perceptron = RandomForestWithPerceptron(n_estimators=100, random_state=42)\n",
        "\n",
        "# Model training\n",
        "\n",
        "rf_with_perceptron.fit(X_train, y_train)\n",
        "\n",
        "# Make a guess\n",
        "\n",
        "y_pred_rf = rf_with_perceptron.predict(X_test)\n",
        "\n",
        "# Report results\n",
        "\n",
        "print(\"Random Forest with Perceptron at each node\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hOYt6JAQhl-",
        "outputId": "8ffb0bf4-a1c2-48a3-d5bf-085d9b5dbc14"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with Perceptron at each node\n",
            "Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      1.00      1.00        21\n",
            "           2       1.00      1.00      1.00        14\n",
            "\n",
            "    accuracy                           1.00        54\n",
            "   macro avg       1.00      1.00      1.00        54\n",
            "weighted avg       1.00      1.00      1.00        54\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest with Perceptron Nodes**\n",
        "\n",
        "**Model Explanation:**\n",
        "\n",
        "In this part of the assignment, we implemented a Random Forest model where each decision node uses a Perceptron (MLP) as its base classifier. A Random Forest is an ensemble method that builds multiple decision trees and merges them together to get a more accurate and stable prediction. By incorporating Perceptron nodes into each tree, we aimed to enhance the model's ability to capture complex patterns in the data.\n",
        "\n",
        "**Advantages of the Model**\n",
        "\n",
        "**Enhanced Learning Capability: **By using Perceptron nodes at each decision point, the model can learn more complex, non-linear relationships within the data, which a simple decision tree might miss.\n",
        "\n",
        "**Robustness:** Random Forest models are known for their robustness and ability to generalize well on unseen data. This is further enhanced by using Perceptron nodes.\n",
        "\n",
        "**Ensemble Power:** Combining multiple Perceptron-based trees in a Random Forest leverages the power of ensemble learning, improving overall model performance and reducing the risk of overfitting.\n",
        "Results and Evaluation:\n",
        "\n",
        "The model was trained and tested on the Wine dataset, yielding excellent results:\n",
        "\n",
        "\n",
        "Random Forest with Perceptron at each node\n",
        "\n",
        "\n",
        "Accuracy: 1.0\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      1.00      1.00        19\n",
        "           1       1.00      1.00      1.00        21\n",
        "           2       1.00      1.00      1.00        14\n",
        "\n",
        "    accuracy                           1.00        54\n",
        "    macro avg       1.00      1.00     1.00        54\n",
        "    weighted avg    1.00      1.00     1.00        54\n",
        "\n",
        "**Evaluation of Results:**\n",
        "\n",
        "**High Performance:** The model achieved 100% accuracy, indicating that all test samples were correctly classified.\n",
        "\n",
        "**Balanced Classification:** The precision, recall, and F1-scores for all classes were 1.00, demonstrating that the model effectively learned and classified all classes in the dataset.\n",
        "\n",
        "**Effective Ensemble Learning:** Using Perceptron nodes within a Random Forest significantly boosted the model's performance compared to simpler models or other ensemble methods. This approach effectively captured the complex patterns in the Wine dataset.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The Random Forest with Perceptron nodes model proved to be extremely effective, achieving perfect classification on the test set. This highlights the power of combining the flexibility of neural networks with the robustness of ensemble methods like Random Forests. The model's ability to learn and generalize complex relationships within the data makes it a powerful tool for classification tasks. These results underscore the importance of selecting the right combination of algorithms and base classifiers to address specific machine learning problems."
      ],
      "metadata": {
        "id": "16uMm6JRcDVU"
      }
    }
  ]
}