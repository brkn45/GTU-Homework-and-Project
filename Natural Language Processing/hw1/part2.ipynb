{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eğitim seti örneği: ['', 'octavianus, mo 31 yilinda ilk zaferini, agrippanin komutasindaki donanmanin birlikleri adriyatik denizinin karsi kiyisina basarili bir sekilde gecirmesiyle kazandi. agrippa, antonius ve kleopatranin ana guclerinin tedarik rotalarini keserken, octavianus da corcyra bugunku korfu adasi karsisinda anakaraya cikti ve guneye dogru yoneldi. karada ve denizde kistirilan antoniusun ordusundamn askerler gun be gun kacarak octavianusun tarafina gecmeye basladilar. octavianusun birlikleri ise rahatca hazirlik yapiyorlardi. antoniusun donanmasi deniz kusatmasini kirabilmek icin umutsuzca yunanistanin bati kiyilarindaki aktium koyuna dogru yelken acti. burada 2 eylul mo 31 tarihinde aktium savasinda antoniusun donanmasi, agrippa ve gaius sosiusun komutasi altindaki kucuk ve manevra kabiliyeti yuksek gemilerden olusan sayica daha buyuk olan filo ile karsilasti. antonius ve kalan birlikleri yakinda bekleyen kleopatranin donanmasinin son andaki cabasiyla kurtuldular. octavianus onlari takip etti ve 1 agustos mo 30 tarihinde iskenderiyede bir kere daha antoniusu yenilgiye ugratti. kleopatra ve antonius intihar ettiler. antonius, sevgilisinin kollarinda kilicyla kendisini oldurdu. kleopatra ise kendisini zehirli bir yilana sokturttu. sezarin vrisi olmasindan gelen konumunu siyasi kariyerinde ilerlemek icin kullanan octavianus bir baskasinin daha ayni konumda bulunmasinin tehlikelerinin farkindaydi. soylendigine gore iki sezar gereginden fazla diyerek caesarionun oldurulmesini emretti. ote yandan kleopatranin antoniustan olan cocuklarinin hayatlarini bagisladi.', 'bahsin 2006 yili nufusu 31,341 kisi ve 6,774 hanedir.']\n",
      "Test seti örneği: ['', 'rakun kopeginin dogal yayilimi sibiryanin dogusu, kuzeydogu cin ve japonyada dir. ancak, 19ncu yuzyilda kurk uretimi icin rusyanin avrupa kismina da getirilmis ve yetistirilmeye baslanmistir ancak rakun kopeginin kurk uretimi icin degerli olan kis postunu yalnizca hur yasarken gelistirdigini tespit etmislerdir. bu yuzden ukraynada 1928 ve 1950 yillari arasinda toplam 10.000 civarinda rakun kopegi dogaya salinmistir. boylece rakun kopegi kendiliginden batiya dogru yayilmaya baslamistir. 1931 yilinda finlandiyaya, 1951 yilinda romanyaya, 1955 yilinda polonyaya ve 1960 yilinda almanyaya ulasmistir. son yillarda fransa ve italyada gozlendigi bildirilmistir.', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_text(file_path):\n",
    "    \"\"\"Loads the Wikipedia text\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text, convert_tr_chars=False):\n",
    "    \"\"\"Converts the text to lowercase and optionally transforms Turkish characters.\"\"\"\n",
    "    text = text.lower()\n",
    "    if convert_tr_chars:\n",
    "        tr_chars = {'ş': 's', 'ç': 'c', 'ğ': 'g', 'ü': 'u', 'ö': 'o', 'ı': 'i'}\n",
    "        text = ''.join([tr_chars.get(c, c) for c in text])\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)  # Remove special character\n",
    "    return text\n",
    "\n",
    "def split_data(text, train_ratio=0.95):\n",
    "    \"\"\"spilit data test and train\"\"\"\n",
    "    sentences = text.split('\\n')\n",
    "    train_data, test_data = train_test_split(sentences, train_size=train_ratio, random_state=42)\n",
    "    return train_data, test_data\n",
    "\n",
    "# 1. Load text\n",
    "text = load_text('wiki_00')\n",
    "\n",
    "# 2. preprocessing\n",
    "processed_text = preprocess_text(text, convert_tr_chars=True)\n",
    "\n",
    "# 3. spilit data test and train\n",
    "train_data, test_data = split_data(processed_text)\n",
    "\n",
    "# Check\n",
    "print(f\"Train Date Set: {train_data[:3]}\")\n",
    "print(f\"Test Data Set: {test_data[:3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate N-gram Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram example: [('o', 12129803), ('c', 6946004), ('t', 13818590), ('a', 37173448), ('v', 3972088), ('i', 42985897), ('n', 23318795), ('u', 15076034), ('s', 16044284), (',', 2725476)]\n",
      "2-gram example: [('oc', 792350), ('ct', 93884), ('ta', 2413665), ('av', 434543), ('vi', 362487), ('ia', 482149), ('an', 5635878), ('nu', 1102275), ('us', 1208659), ('s,', 131634)]\n",
      "3-gram example: [('oct', 3636), ('cta', 25327), ('tav', 14928), ('avi', 45873), ('via', 4946), ('ian', 44727), ('anu', 32061), ('nus', 90131), ('us,', 21790), ('s, ', 129673)]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    \"\"\"Generates n-gram character sequences from the given text.\"\"\"\n",
    "    ngrams = [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "    return ngrams\n",
    "\n",
    "def build_ngram_model(data, n):\n",
    "    \"\"\"Creates an n-gram model for the given data.\"\"\"\n",
    "    ngram_counts = defaultdict(int)\n",
    "    for sentence in data:\n",
    "        ngrams = generate_ngrams(sentence, n)\n",
    "        for ngram in ngrams:\n",
    "            ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "# Generate 1, 2, and 3-gram tables from the training data.\n",
    "unigram_model = build_ngram_model(train_data, 1)\n",
    "bigram_model = build_ngram_model(train_data, 2)\n",
    "trigram_model = build_ngram_model(train_data, 3)\n",
    "\n",
    "# Example Outputs\n",
    "print(f\"1-gram example: {list(unigram_model.items())[:10]}\")\n",
    "print(f\"2-gram example: {list(bigram_model.items())[:10]}\")\n",
    "print(f\"3-gram example: {list(trigram_model.items())[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Good Turing Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram good turing smoothing: [('o', 0.031125663001181834), ('c', 0.017823783264152022), ('t', 0.03545917237827368), ('a', 0.09538887111686453), ('v', 0.010192570522294414)]\n",
      "2-gram good turing smoothing: [('oc', 0.00204499011283994), ('ct', 0.00024230687417664532), ('ta', 0.006229470638868952), ('av', 0.0011215197054380084), ('vi', 0.0009355491020799033)]\n",
      "3-gram good turing smoothing: [('oct', 9.438859491704645e-06), ('cta', 6.574752319758072e-05), ('tav', 3.875228121346725e-05), ('avi', 0.00011908382878519448), ('via', 1.2839548692511323e-05)]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_frequency_of_frequencies(ngram_counts):\n",
    "    \"\"\"Calculate the frequency of frequencies based on n-gram frequencies.\"\"\"\n",
    "    freq_of_freqs = defaultdict(int)\n",
    "    for freq in ngram_counts.values():\n",
    "        freq_of_freqs[freq] += 1\n",
    "    return freq_of_freqs\n",
    "\n",
    "def good_turing_smoothing(ngram_counts, total_ngrams):\n",
    "    \"\"\"Calculate the adjusted probabilities using Good-Turing smoothing.\"\"\"\n",
    "    smoothed_probs = {}\n",
    "    freq_of_freqs = compute_frequency_of_frequencies(ngram_counts)\n",
    "    \n",
    "    for ngram, count in ngram_counts.items():\n",
    "        c_next = freq_of_freqs.get(count + 1, 0)\n",
    "        if c_next > 0:\n",
    "            adjusted_count = (count + 1) * c_next / freq_of_freqs[count]\n",
    "        else:\n",
    "            adjusted_count = count  # Eğer c+1 frekansı yoksa, orijinal değeri al\n",
    "        smoothed_probs[ngram] = adjusted_count / total_ngrams\n",
    "\n",
    "    return smoothed_probs\n",
    "\n",
    "# 1. Apply smoothing for each n-gram model.\n",
    "total_unigrams = sum(unigram_model.values())\n",
    "total_bigrams = sum(bigram_model.values())\n",
    "total_trigrams = sum(trigram_model.values())\n",
    "\n",
    "unigram_probs = good_turing_smoothing(unigram_model, total_unigrams)\n",
    "bigram_probs = good_turing_smoothing(bigram_model, total_bigrams)\n",
    "trigram_probs = good_turing_smoothing(trigram_model, total_trigrams)\n",
    "\n",
    "# 2. Example Outputs\n",
    "print(f\"1-gram good turing smoothing: {list(unigram_probs.items())[:5]}\")\n",
    "print(f\"2-gram good turing smoothing: {list(bigram_probs.items())[:5]}\")\n",
    "print(f\"3-gram good turing smoothing: {list(trigram_probs.items())[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram perplexity: 20.893676782582887\n",
      "2-gram perplexity: 242.4144641543308\n",
      "3-gram perplexity: 2006.9914560059249\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_perplexity(test_data, ngram_probs, n):\n",
    "    \"\"\"Verilen test verisi için perplexity hesaplar.\"\"\"\n",
    "    log_prob_sum = 0\n",
    "    total_ngrams = 0\n",
    "\n",
    "    for sentence in test_data:\n",
    "        ngrams = generate_ngrams(sentence, n)\n",
    "        for ngram in ngrams:\n",
    "            prob = ngram_probs.get(ngram, 1e-8)  #A very small probability for unseen n-grams.\n",
    "            log_prob_sum += math.log(prob)\n",
    "            total_ngrams += 1\n",
    "\n",
    "    perplexity = math.exp(-log_prob_sum / total_ngrams)\n",
    "    return perplexity\n",
    "\n",
    "# 1. Calculate perplexity (for each model)\n",
    "unigram_perplexity = calculate_perplexity(test_data, unigram_probs, 1)\n",
    "bigram_perplexity = calculate_perplexity(test_data, bigram_probs, 2)\n",
    "trigram_perplexity = calculate_perplexity(test_data, trigram_probs, 3)\n",
    "\n",
    "# 2. Print the results.\n",
    "print(f\"1-gram perplexity: {unigram_perplexity}\")\n",
    "print(f\"2-gram perplexity: {bigram_perplexity}\")\n",
    "print(f\"3-gram perplexity: {trigram_perplexity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram :  avaoavottvoavtacotvcoaatcccattvttcttvcttactvocvoa\n",
      "1-gram : hvovaavtvaavcoacacvvcctvattavoaooaaoaooatoavttavcv\n",
      "2-gram : 64 senugutini,35730 z ia ia yrmavavgagunippinagrlk\n",
      "2-gram : ldipaftopi,54 mendon moplaviaftrliagrs 392 ilagugi\n",
      "3-gram : aysaynanmeklon tir klikisek dri asinium decelkilis\n",
      "3-gram :  akaltarindi. agrona asi,olulundeclanizdilislyyist\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_sentence(ngram_probs, n, max_length=50):\n",
    "    \"\"\"N-gram modeli kullanarak rastgele bir cümle üretir.\"\"\"\n",
    "    sentence = \"\"\n",
    "    current_ngram = random.choice(list(ngram_probs.keys()))  # Random start\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        sentence += current_ngram[-1]  # Append the last character of the n-gram.\n",
    "\n",
    "        # Starting from the current n-gram, find the appropriate n-grams for the next step.\n",
    "        candidates = [ngram for ngram in ngram_probs if ngram.startswith(current_ngram[1:])]\n",
    "        \n",
    "        if not candidates:\n",
    "            break  # If there are no suitable n-grams left, end the sentence.\n",
    "        \n",
    "        # Make a random selection from the first 5 n-grams\n",
    "        next_ngram = random.choice(candidates[:5])\n",
    "        current_ngram = next_ngram\n",
    "\n",
    "    return sentence\n",
    "\n",
    "# Generate random sentences for each model.\n",
    "print(\"1-gram :\", generate_sentence(unigram_probs, 1))\n",
    "print(\"1-gram :\", generate_sentence(unigram_probs, 1))\n",
    "print(\"2-gram :\", generate_sentence(bigram_probs, 2))\n",
    "print(\"2-gram :\", generate_sentence(bigram_probs, 2))\n",
    "print(\"3-gram :\", generate_sentence(trigram_probs, 3))\n",
    "print(\"3-gram :\", generate_sentence(trigram_probs, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
