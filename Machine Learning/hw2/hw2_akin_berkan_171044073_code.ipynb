{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_J3-XSciQqh",
        "outputId": "a76a2d5f-79e1-4906-e4fd-067f89077dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Native tree: 0.08492822966507177\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Load the abalone dataset from the newly uploaded file\n",
        "abalone_data = 'abalone.data'\n",
        "columns = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\", \"Viscera weight\", \"Shell weight\", \"Rings\"]\n",
        "abalone_data_new = pd.read_csv(abalone_data, header=None, names=columns)\n",
        "\n",
        "\n",
        "# Convert the 'Sex' column to numerical values\n",
        "sex_encoder = LabelEncoder()\n",
        "abalone_data_new['Sex'] = sex_encoder.fit_transform(abalone_data_new['Sex'])\n",
        "\n",
        "# Show the first few rows of the newly loaded and processed dataset to verify changes\n",
        "abalone_data_new.head()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = abalone_data_new.drop('Rings', axis=1).values\n",
        "y = abalone_data_new['Rings'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# Redefine the decision tree functions with necessary imports\n",
        "def calculate_entropy(y):\n",
        "    if len(y) == 0:\n",
        "        return 0\n",
        "    class_labels, counts = np.unique(y, return_counts=True)\n",
        "    probabilities = counts / counts.sum()\n",
        "    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-9))  # avoid log2(0)\n",
        "    return entropy\n",
        "\n",
        "def calculate_information_gain(X, y, feature_index):\n",
        "    total_entropy = calculate_entropy(y)\n",
        "    values, counts = np.unique(X[:, feature_index], return_counts=True)\n",
        "    weighted_entropy = sum((count / y.size) * calculate_entropy(y[X[:, feature_index] == value]) for value, count in zip(values, counts))\n",
        "    return total_entropy - weighted_entropy\n",
        "\n",
        "def build_tree(X, y, features, depth=0, max_depth=5):\n",
        "    if len(np.unique(y)) == 1 or len(features) == 0 or depth == max_depth:\n",
        "        return np.bincount(y).argmax() if y.size > 0 else None\n",
        "    gains = [calculate_information_gain(X, y, feature) for feature in features]\n",
        "    best_feature = features[np.argmax(gains)]\n",
        "    tree = {best_feature: {}}\n",
        "    features = [f for f in features if f != best_feature]\n",
        "    for value in np.unique(X[:, best_feature]):\n",
        "        sub_X = X[X[:, best_feature] == value]\n",
        "        sub_y = y[X[:, best_feature] == value]\n",
        "        subtree = build_tree(sub_X, sub_y, features, depth + 1, max_depth)\n",
        "        tree[best_feature][value] = subtree\n",
        "    return tree\n",
        "\n",
        "def predict_dt(tree, x_test):\n",
        "    for key, value in tree.items():\n",
        "        feature_val = x_test[key]\n",
        "        if feature_val in value:\n",
        "            result = value[feature_val]\n",
        "            if isinstance(result, dict):\n",
        "                return predict_dt(result, x_test)\n",
        "            else:\n",
        "                return result\n",
        "    return None\n",
        "\n",
        "def print_tree(tree, depth=0):\n",
        "    \"\"\" Recursively print the decision tree in a formatted manner \"\"\"\n",
        "    if not isinstance(tree, dict):\n",
        "        print(\"\\t\" * depth + \"-> Prediction: \" + str(tree))\n",
        "        return\n",
        "    for feature_index, branches in tree.items():\n",
        "        for value, subtree in branches.items():\n",
        "            print(\"\\t\" * depth + \"Feature {}: {}\".format(feature_index, value))\n",
        "            print_tree(subtree, depth + 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Rebuild the decision tree and evaluate it\n",
        "features_indices = list(range(X.shape[1]))\n",
        "tree = build_tree(X_train, y_train, features_indices, max_depth=5)\n",
        "test_predictions = [predict_dt(tree, dict(enumerate(x))) for x in X_test]\n",
        "accuracy = np.mean(np.array(test_predictions) == y_test)\n",
        "#print_tree(tree,0)\n",
        "print(\"Native tree:\",accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Buiid Tree And Predict Tree**\n",
        "\n",
        "The above code aims to build and evaluate a decision tree model for predicting the age of abalones using the Abalone dataset. Initially, the code loads the dataset and converts the 'Sex' column into numerical values, making it processable for machine learning algorithms. After splitting the data into training and testing sets, the user defines their own decision tree functions to calculate entropy and information gain, and builds the tree based on these values.\n",
        "\n",
        "As a result, the decision tree model achieves a very low accuracy rate of 8.49% on the test dataset. This indicates that the model fails to generalize well and is unsuccessful in making accurate predictions. The low accuracy could suggest that the model might be overfitting, or that the dataset may not be suitable for modeling with a decision tree algorithm. To address this issue, one could try different parameter settings, add more data preprocessing steps to better prepare the data for modeling, or perhaps use a different algorithm."
      ],
      "metadata": {
        "id": "gjISTAsFllqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(tree, X, y):\n",
        "    predictions = [predict_dt(tree, dict(enumerate(row))) for row in X]\n",
        "    return np.mean(np.array(predictions) == y)\n",
        "\n",
        "def prune_tree(tree, X_valid, y_valid):\n",
        "    \"\"\" Prune the decision tree based on reduced error pruning strategy using the validation set. \"\"\"\n",
        "    if not isinstance(tree, dict):\n",
        "        return  # If the tree is not a dictionary, it's a leaf node, return immediately\n",
        "\n",
        "    # Try to prune each subtree recursively\n",
        "    for feature in list(tree.keys()):\n",
        "        subtrees = tree[feature]\n",
        "        for value, subtree in list(subtrees.items()):\n",
        "            sub_X_valid = X_valid[X_valid[:, feature] == value]\n",
        "            sub_y_valid = y_valid[X_valid[:, feature] == value]\n",
        "\n",
        "            if isinstance(subtree, dict) and sub_y_valid.size > 0:  # Ensure there are samples to work with\n",
        "                prune_tree(subtree, sub_X_valid, sub_y_valid)\n",
        "\n",
        "                # Check if replacing the subtree with a leaf improves the accuracy\n",
        "                leaf_value = np.bincount(sub_y_valid).argmax() if sub_y_valid.size > 0 else np.bincount(y_valid).argmax()\n",
        "                subtree_as_leaf = leaf_value\n",
        "                tree_with_leaf = {feature: {value: subtree_as_leaf}}\n",
        "\n",
        "                # Calculate accuracy with the subtree replaced by the leaf\n",
        "                current_accuracy = calculate_accuracy(tree, X_valid, y_valid)\n",
        "                new_accuracy = calculate_accuracy(tree_with_leaf, X_valid, y_valid)\n",
        "\n",
        "                # If accuracy is improved or stays the same, replace the subtree with the leaf\n",
        "                if new_accuracy >= current_accuracy:\n",
        "                    tree[feature][value] = subtree_as_leaf\n",
        "\n",
        "prune_tree(tree, X_train, y_train)\n",
        "\n",
        "pruned_accuracy = calculate_accuracy(tree, X_test, y_test)\n",
        "print(\"Pruned tree:\",pruned_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rC66JizZTH7Z",
        "outputId": "0b737cb8-dc0c-4074-97f1-f95544788924"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned tree: 0.08492822966507177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pruned Tree**\n",
        "\n",
        "The above code attempts to improve the performance of a decision tree model by pruning the tree using a reduced error pruning strategy with a validation set. The code iteratively evaluates each node of the tree, and if replacing a particular subtree with a leaf node maintains or improves accuracy, it replaces that subtree with the leaf value. This process is intended to help prevent overfitting and enhance the model's generalization capability.\n",
        "\n",
        "However, according to the output, the pruning process has not effectively increased the model's accuracy, remaining at 8.49%. This might indicate that the pruning strategy used may not be suitable for the current dataset and the structure of the problem, or that the model was already too simple for pruning to have a significant effect. To develop a more effective modeling strategy, it may be worth considering different machine learning techniques or improving the data preprocessing steps."
      ],
      "metadata": {
        "id": "aYbRa5fOmlv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rdf(X, y, attribute_types, N, options):\n",
        "    trees = []\n",
        "    for _ in range(N):\n",
        "        # Bootstrap sample\n",
        "        indices = np.random.choice(np.arange(len(X)), size=len(X), replace=True)\n",
        "        sample_X = X[indices]\n",
        "        sample_y = y[indices]\n",
        "        tree = build_tree(sample_X, sample_y, attribute_types, options.get('max_depth', 10))\n",
        "        trees.append(tree)\n",
        "    return trees\n",
        "\n",
        "def predict_single(tree, x):\n",
        "    while isinstance(tree, dict):\n",
        "        if x[tree['attribute']] <= tree['threshold']:\n",
        "            tree = tree['left']\n",
        "        else:\n",
        "            tree = tree['right']\n",
        "    return tree\n",
        "\n",
        "def predict_rdf(rdf, X, options):\n",
        "    predictions = []\n",
        "    for x in X:\n",
        "        votes = [predict_single(tree, x) for tree in rdf]\n",
        "        predictions.append(max(set(votes), key=votes.count))\n",
        "    return predictions\n",
        "\n",
        "# Build the Random Decision Forest\n",
        "rdf = build_rdf(X_train, y_train, features_indices, N=10, options={'max_depth': 5})\n",
        "\n",
        "# Predict and evaluate the model\n",
        "predictions = predict_rdf(rdf, X_test, options={})\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print('RDF Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFOTCibqTbgQ",
        "outputId": "9bd002fd-44e2-485c-806c-8811c40dcbf6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDF Accuracy: 0.16985645933014354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RDF Tree**\n",
        "\n",
        "The code snippet above performs a series of operations to construct a Random Decision Forest (RDF) model. This process involves drawing bootstrap samples from the training data to independently construct each tree, using parameters such as the indices of attributes and maximum depth. Each sample is then used to build a decision tree, which is added to a list to form the forest. During the prediction phase, predictions are made for each test instance across all trees in the forest, and the most frequent prediction is selected as the final result.\n",
        "\n",
        "As a result, the model achieves an accuracy of 16.99% on the test dataset. This indicates that the model still performs poorly, but there is an improvement over the previous decision tree model. Although the Random Decision Forest method generally has better generalization capabilities compared to a single decision tree, the accuracy remains quite low in this case. To enhance performance, a forest with more trees could be utilized, the depth of the trees could be increased, or different feature selection and data preprocessing techniques could be explored."
      ],
      "metadata": {
        "id": "PzI9zmBRmqTt"
      }
    }
  ]
}